---
title: "On the Convergence Theory of Gradient-Based Model-Agnostic Meta-Learning Algorithms"
path: "/convergence-maml"
tags: ["Paper Note"]
featuredImage: "./cover.jpg"
excerpt: Paper summery about the convergence of MAML and newly proposed algorithms.
created: 2019-12-08
updated: 2019-12-08
---

Paper URL: <a href="https://arxiv.org/abs/1908.10400">https://arxiv.org/abs/1908.10400</a>

# TLDR
In this paper, the authors analyze Model Agnostic Meta Learning (MAML) from [this paper](https://arxiv.org/abs/1703.03400),
which includes the original MAML algorithms, and its first order approximation. They concluded that MAML is converging, however, its first order approximation doesn't in general unless some condition applies. Furthermore, they proposed Hessian-Free MAML that is faster to compute (since it doesn't require Hessian computation) compare to MAML but still converge.


## Background

### MAML Objective

### MAML Algorithm

### MAML First Order Approximation



## Results

### Hessian Free MAML

### Theoretical Analysis of MAML


*Cover by [@seanpaulkinnear](https://unsplash.com/@seanpaulkinnear) Thanks*
